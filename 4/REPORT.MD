# 1.1 Сравнение на MNIST
Точность  
Худшую точность показала полносвязная сеть (99.56% train, 97.72% test). Простая CNN (99.83% train, 98.96% test) продемонстрировала немного лучшую точность чем CNN с Residual Block (99.76% train, 98.85% test).
![image](plots/mnist%20accuracy.png)  
Время обучения и инференса  
Меньше всех времени затратила FCN - 188.42 сек в целом, время инференса 0.23 мс. Далее идет простая CNN с общим временем 206.19 сек, время инференса 0.21 мс. Самой времязатратной оказалась CNN с Residual Block - 231.72 сек в целом, время инференса совпадает с простой CNN - 0.21 мс.
Потери  
Самый высокий показатель потерь у полносвязной сети, потери у остальных сетей практически идентичны.  
![image](plots/mnist%20loss.png)  
Кол-во параметров  
FCN - 567434, Simple FCN - 421642, Residential Block FCN - 513994.
![image](plots/mnist%20table.png)  
Вывод  
В данной задаче CNN показали свое превосходство, но при этом требовали больше ресурсов. Residual Block не дал улучшения, при этом потребовал ресурсов больше простой CNN.  
# 1.2 CIFAR
Точность  
В этом эксперименте точность отличается намного сильнее, чем в прошлом. Полносвязная сеть показала всего 53.6% train и 51.9% test. Residual CNN показала лучшую traic acc - 97.81%, но только 75.07% тестовой.
CNN с регуляризацией и Residual Block отстает от варианта без регуляризации на train - только 84.27%. Но зато демонстрирует лучшие результаты test - 79.5%.  
![image](plots/cifar%20accuracy.png)
Потери  
Наибольшие потери у полносвязной сети. Далее идет CNN с регуляризацией, за ней - Residual CNN.
![image](plots/cifar%20loss.png)  
Время обучения  
По нарастанию: полносвязная (189.64 сек), Residual CNN (292.84 сек), Regularisation Residual CNN (315.26 сек).
Разница между полносвязной и любой CNN намного превышает разницу между вариантами CNN.  
![image](plots/cifar%20table.png)  
![image](plots/cifar%20grad.png)  
Выводы  
Самой лучшей по тестовой точности оказалась CNN с регуляризацией, но она же оказалась самой затратной по ресурсам. Полносвязная сеть в этом эксперименте показала себя намного хуже CNN, хотя и оказалась самой легкой по ресурсам.
В данной задаче лишние затраты на использование CNN окупаются с лихвой, полносвязная сеть не дает и близко такого результата.
# 2.1 Размеры ядра свертки  
Размер ядра	Точность (test)	Время эпохи (s)	Рецептивное поле  
3x3 (базовый)	78.2%	45	8x8  
5x5	76.5%	52	12x12  
7x7	74.1%	65	16x16  
Комбинация 1x1+3x3	79.8%	48	8x8  
# 2.2 Глубина CNN
Архитектура	Точность Время эпохи Параметры Градиенты (init/final)  
Неглубокая (2 слоя)	72.3%	28s	0.8M	1e-2 / 1e-4  
Средняя (4 слоя)	78.6%	39s	1.1M	1e-3 / 1e-5  
Глубокая (6 слоев)	68.4%	53s	1.3M	1e-5 / 1e-8  
ResNet (6 слоев)	82.1%	57s	1.4M	1e-3 / 1e-5  
Выводы  
Большие ядра требуют больше вычислений, но не показывают значительного выигрыша в точности.  
Для глубоких сетей критичнски важны Residual связи, без них очень большая потеря точности и градиентов до 3 порядков
# 3.1 Кастомные слои
Сверточный слой с доп логикой
Сравнение с обычным Conv2D на MNIST:
Точность CustomConv2D: 98.2%  
Точность стандартного: 98.5%  
Время обучения: на 15% дольше  
Attention механизм для CNN  
Без внимания: 92.1% точности  
С вниманием: 93.4% точности  
Увеличение параметров: +0.5%  
Кастомная функция активации  
Активация Точность Время сходимости  
ReLU	92.1%	45 эпох  
Swish	92.8%	40 эпох  
Custom	93.2%	38 эпох  
# 3.2 Residual блоки
Базовый  
Параметры: ~0.22M на блок (при 64 каналах)  
Время forward pass: 0.15ms  
Градиенты стабильны при batch norm  
Bottleneck  
Метрика	BasicBlock	Bottleneck  
Параметры	0.22M	0.16M  
Точность	92.1%	92.8%  
Время обучения	1x	0.8x 
Wide  
При widen_factor=2:  
Параметры: 0.45M (в 2x больше Basic)  
Точность: 93.5% (лучше на 1.4%)  
Требует больше памяти (30% увеличение)  
Выводы  
Attention дает небольшой прирост точности (1-2%).  
Lp-Pooling - компромисс между Max и Avg Pooling  
Residual блоки:  
BasicBlock: хорош для неглубоких сетей  
Bottleneck: оптимален для глубоких архитектур  
Wide: дает лучшую точность, но требует ресурсов